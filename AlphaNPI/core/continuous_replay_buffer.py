import numpy as np
from scipy.stats import beta as BetaDist

class PrioritizedReplayBuffer():
    '''
    This class represents a replay buffer memory in which traces generated by the MCTS are stored.
    '''

    def __init__(self, max_length, task_ids, alpha=2.5, beta=0.5):
        self.task_ids = task_ids
        self.memory_task = dict((task_id, []) for task_id in self.task_ids)
        self.stack = []
        self.max_length = max_length
        self.alpha = alpha
        self.beta = beta

    def get_memory_length(self):
        return len(self.stack)

    def append_trace(self, trace):
        '''
        Add a newly generated execution trace to the memory buffer.

        Args:
            trace: a sequence of [(e_0, i_0, (h_0, c_0), pi_0, r_0), ... , (e_T, i_T, (h_T, c_T), pi_T, r_T)]
        '''
        for tup in trace:
            if len(self.stack) >= self.max_length:
                t_id = self.stack[0][1]
                r = max(0, self.stack[0][4])
                del self.memory_task[t_id][r][0]
                del self.stack[0]
            task_id = tup[1]
            self.memory_task[task_id].append(tup)
            self.stack.append(tup)

    def _sample_sub_batch(self, batch_size, memory):
        indices = np.arange(len(memory))
        sampled_indices = np.random.choice(indices, size=batch_size, replace=(batch_size > len(memory)))
        batch = [[], [], [], [], [], []]
        for i in sampled_indices:
            for k in range(6):
                batch[k].append(memory[i][k])

        return batch

    def sample_batch(self, batch_size):
        '''
        Sample in the memory a batch of experience.

        Args:
            batch_size: the batch size

        Returns:
            a list [batch of e_t, batch of i_t, batch of (h_t, c_t), batch of pi_t, batch of r_t]
        '''

        memory = []
        rewards = []
        for task_id in self.memory_task:
            task_rewards = np.array([tup[4] for tup in self.memory_task[task_id]])
            if (task_rewards == 0.0).any():
                reward_1_idx = np.argwhere(task_rewards == 0.0).tolist()
                memory += self.memory_task[task_id][reward_1_idx]
                rewards += task_rewards[reward_1_idx]
            else:
                memory += self.memory_task[task_id]
                rewards += task_rewards.tolist()

        rewards = [-r/10 for r in rewards]

        reward_sample_probs = BetaDist.pdf(rewards, self.alpha, self.beta)
        reward_sample_probs[reward_sample_probs != reward_sample_probs] = 0.0
        reward_sample_probs = reward_sample_probs/reward_sample_probs.sum()
        indices = np.arange(len(memory))
        sampled_indices = np.random.choice(indices, size=batch_size, replace=(batch_size > len(memory)), p=reward_sample_probs)
        batch = [[], [], [], [], [], []]
        for i in sampled_indices:
            for k in range(6):
                batch[k].append(memory[i][k])

        return batch if batch else None

    def empty_memory(self):
        '''
        Empty the replay memory.
        '''
        self.memory_task = dict((task_id, []) for task_id in self.task_ids)
        self.stack = []
